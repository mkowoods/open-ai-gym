# sample code: https://github.com/erilyth/Universe-RL-Samples/blob/master/cartpole-v0.py

#! /usr/bin/env python
# -*- coding: utf-8 -*-
# vim:fenc=utf-8
#
# Copyright Â© 2016 vishalapr <vishalapr@vishal-Lenovo-G50-70>
#
# Distributed under terms of the MIT license.

"""

"""

import gym
import universe

import time
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import sgd

load_model = 1
backup_iter = 500
save_iter = 5
memory_clear = 100

# Reinforcement Learning - Deep-Q learning
model = Sequential()
model.add(Dense(128, input_dim=4, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(2))
model.compile(loss='mse', optimizer=sgd(lr=0.0001))

if load_model == 1:
	model.load_weights("cartpole-v0.keras")

env = gym.make('CartPole-v0')
env.monitor.start('/tmp/cartpole-experiment-1', force=True)
replay_memory = []
gamma = 0.9 # Future reward decrement
epsilon = 0.2 # Probability of selecting random action
epsilon_min = 0.0 # Minimum random action selection probability
episodes = 500
epsilon_decay = (epsilon - epsilon_min) / episodes # Random action selection probability decay

for episode in range(episodes):
	observation = env.reset()
	observation = np.reshape(observation, [1, 4])
	for time_t in range(5000):
		print epsilon
		env.render()
		# Action space is either 0 or 1 for cartpole
		# print env.action_space
		action = model.predict(observation)
		print action
		action = np.argmax(action[0])
		if np.random.uniform(0,1) < epsilon:
			# Either 0 or 1 sample the action randomly
			action = np.random.randint(2)
		#action = env.action_space.sample()
		# print "Action:", action
		observation_old = observation
		observation, reward, done, info = env.step(action)
		observation = np.reshape(observation, [1, 4])
		print observation
		replay_memory.append([observation_old, action, reward, observation])
		if done:
			print 'Episode finished'
			break
	print "Replay Memory Size:", len(replay_memory)
	indices = np.random.choice(len(replay_memory), min(500, len(replay_memory)))
	for mem_idx in indices:
		mem = replay_memory[mem_idx]
		observation_old = mem[0]
		action = mem[1]
		reward = mem[2]
		observation = mem[3]
		target = reward
		if mem_idx != len(replay_memory) - 1:
			target = reward + gamma * np.amax(model.predict(observation)[0])
		print "Target:", target
		target_f = model.predict(observation_old)
		target_f[0][action] = target
		model.fit(observation_old, target_f, nb_epoch=1, verbose=0)
	if episode % save_iter == 0:
		model.save_weights("cartpole-v0.keras")
	if episode % backup_iter == 0:
		model.save_weights("cartpole_backup" + str(episode) + "-v0.keras")
	if episode % memory_clear == 0:
		replay_memory = []
	epsilon -= epsilon_decay

env.monitor.close()
# Upload onto gym
last_chars = raw_input("Enter last two characters of key: ")
gym.scoreboard.api_key = 'sk_Ai0CaXYKRRS4XX5mCdlJ' + last_chars
gym.upload('/tmp/cartpole-experiment-1')